{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:14.907488Z","iopub.status.busy":"2022-10-05T21:30:14.907104Z","iopub.status.idle":"2022-10-05T21:30:14.912434Z","shell.execute_reply":"2022-10-05T21:30:14.911413Z","shell.execute_reply.started":"2022-10-05T21:30:14.907460Z"},"trusted":true},"outputs":[],"source":["\n","!pip install mrmr_selection --quiet\n","!pip install skfeature-chappers --quiet\n","!pip install ReliefF --quiet\n","!pip install imbalanced-learn --quiet\n","!pip install scikit-posthocs --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:14.918441Z","iopub.status.busy":"2022-10-05T21:30:14.916914Z","iopub.status.idle":"2022-10-05T21:30:14.927546Z","shell.execute_reply":"2022-10-05T21:30:14.925945Z","shell.execute_reply.started":"2022-10-05T21:30:14.918382Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import heapq\n","import numpy as np\n","from sklearn.feature_selection import SelectFdr, RFE,f_classif\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVC, SVR, SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","import csv\n","import scipy\n","import scipy.io\n","\n","from sklearn.model_selection import LeavePOut,LeaveOneOut, KFold\n","from sklearn.model_selection import cross_validate\n","\n","# Feature selection preprocess\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.preprocessing import PowerTransformer\n","from sklearn.impute import SimpleImputer\n","\n","from sklearn.feature_selection import SelectKBest\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:14.929981Z","iopub.status.busy":"2022-10-05T21:30:14.929611Z","iopub.status.idle":"2022-10-05T21:30:14.951369Z","shell.execute_reply":"2022-10-05T21:30:14.950564Z","shell.execute_reply.started":"2022-10-05T21:30:14.929945Z"},"trusted":true},"outputs":[],"source":["from sklearn import preprocessing\n","from sklearn.impute import SimpleImputer\n","from scipy.io.arff import loadarff\n","\n","# Duplicating a single instance for the sake of stratifiedKFold\n","def duplicate_single_instances(df):\n","    labels = list(df.iloc[:,-1])\n","    unique_labels, counts = np.unique(labels,return_counts=True)\n","    labels_once = [unique_labels[i] for i in range(len(counts)) if counts[i] ==1 ]\n","    for x in labels_once:\n","        row = df[df.iloc[:,-1] == x]\n","        df = df.append(row,ignore_index=True)\n","        \n","    return df\n","\n","# Converting labels to [1....n]\n","def classes_to_1_and_above(df):\n","    labels = df.iloc[:,-1]\n","    if type(labels[0]) is bytes:\n","        labels = [l.decode() for l in labels]\n","    if type(labels[0]) is str:\n","        unique_labels = np.unique(labels)\n","        new_labels = [np.where(unique_labels == x)[0][0] + 1 for x in labels]\n","    else:\n","        le = preprocessing.LabelEncoder()\n","        new_labels = le.fit_transform(labels)\n","        new_labels = [x+1 for x in new_labels]\n","    df.iloc[:,-1] = new_labels\n","\n","# Not using\n","def fill_labels(df):\n","    labels = df.iloc[:,-1]\n","    maxl = labels[0] # find max label\n","    if labels.isnull().values.any():\n","        for label in labels:\n","            if label != np.nan:\n","                if maxl == np.nan:\n","                    maxl = label\n","                else:\n","                    maxl = max(maxl,label)\n","        df.iloc[:,-1].fillna(maxl + 1,inplace=True) # fill holes in max label + 1 and treat it as a class\n","    \n","def impute(df):\n","    mean = df.mean()\n","    df.fillna(mean,inplace=True)\n","    \n","# Main funciton for performing preprocessing\n","# Inversing, turning first row to header, move first column to last and deleting first column if needed.\n","# Additionaly, perofrming imputation and convertion of labels to [1....n]\n","def perform_df_processing(df,inv=False, FirstRowToHeader=False, putFirstLast=False,DeleteFirst=False):\n","    if inv:\n","        df=df.T\n","    if FirstRowToHeader:\n","        df.columns = df.iloc[0] # Removing the first row of rows names\n","        df = df.drop(df.index[0])\n","    if putFirstLast:\n","        temp_cols=df.columns.tolist()\n","        new_cols = temp_cols[1:-1] + temp_cols[0:1]\n","        df=df[new_cols]\n","    if DeleteFirst:\n","        colname = df.columns[0]\n","        df.drop(columns=[colname],inplace=True)\n","    \n","    df = duplicate_single_instances(df)\n","    impute(df)\n","    classes_to_1_and_above(df)\n","    return df\n","\n","def read_csv(name,inv=False, FirstRowToHeader=False, putFirstLast=False,DeleteFirst=False):\n","    df = pd.read_csv(name)\n","    return perform_df_processing(df,inv,FirstRowToHeader, putFirstLast,DeleteFirst)\n","\n","def read_mat(name,inv=False, FirstRowToHeader=False, putFirstLast=False,DeleteFirst=False):\n","    mat = scipy.io.loadmat(name)\n","    df = pd.DataFrame(np.hstack((mat['X'], mat['Y'])))\n","    return perform_df_processing(df,inv,FirstRowToHeader, putFirstLast,DeleteFirst)\n","\n","def read_arff(name,inv=False, FirstRowToHeader=False, putFirstLast=False,DeleteFirst=False):\n","    data = loadarff(name)\n","    df = pd.DataFrame(data[0])\n","    return perform_df_processing(df,inv,FirstRowToHeader,putFirstLast,DeleteFirst)\n","#     return df\n","\n","# General read dataset file which reads all relevant kinds of dataset files\n","def read(name,inv=False, FirstRowToHeader=False, putFirstLast=False,DeleteFirst=False):\n","    if name.endswith(\"mat\"):\n","        return read_mat(name,inv,FirstRowToHeader, putFirstLast,DeleteFirst)\n","    elif name.endswith(\"csv\"):\n","        return read_csv(name,inv,FirstRowToHeader, putFirstLast, DeleteFirst)\n","    elif name.endswith(\"arff\"):\n","        return read_arff(name,inv,FirstRowToHeader, putFirstLast, DeleteFirst)\n","    \n","def get_x_y(df):\n","    return df.iloc[:,0:-1], df.iloc[:,-1].astype('int')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# WRITING FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:14.953162Z","iopub.status.busy":"2022-10-05T21:30:14.952851Z","iopub.status.idle":"2022-10-05T21:30:14.975823Z","shell.execute_reply":"2022-10-05T21:30:14.974065Z","shell.execute_reply.started":"2022-10-05T21:30:14.953137Z"},"trusted":true},"outputs":[],"source":["from os.path import exists\n","csv_top = ['Dataset Name',\n","          'Number of samples', 'Original number of features', \n","          'Filtering Algorithm', 'Learning Algorithm', \n","          'Number of features selected (K)', 'CV Method',\n","          'Fold', 'Measure Type', \n","          'Measure Value', 'List of Selected Features Names', \n","          'Selected Features Scores', 'Training time', \n","          'Testing time']\n","\n","def write_to_results(   ds_name,\n","                        n_samples,\n","                        original_n_features,\n","                        filtering_algo, \n","                        learning_algo,\n","                        n_features_selected,\n","                        cv_method,\n","                        fold,\n","                        measure_type, \n","                        measure_value,\n","                        selected_features_names,\n","                        selected_features_scores, \n","                        train_time,\n","                        test_time):\n","    \n","    row = [\n","            ds_name,\n","            n_samples,\n","            original_n_features,\n","            filtering_algo,\n","            learning_algo,\n","            n_features_selected,\n","            cv_method,\n","            fold,\n","            measure_type,\n","            measure_value,\n","            selected_features_names,\n","            selected_features_scores,\n","            train_time,\n","            test_time]\n","    \n","    if (not exists(ds_name + \".csv\")):\n","      with open(ds_name+\".csv\", 'w+', encoding='UTF8') as ds_file:\n","        writer = csv.writer(ds_file)\n","        writer.writerow(csv_top)\n","    with open(ds_name+ \".csv\", 'a', encoding='UTF8', newline='') as ds_file:\n","        writer = csv.writer(ds_file)\n","        writer.writerow(row)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:14.978227Z","iopub.status.busy":"2022-10-05T21:30:14.977712Z","iopub.status.idle":"2022-10-05T21:30:14.997816Z","shell.execute_reply":"2022-10-05T21:30:14.996038Z","shell.execute_reply.started":"2022-10-05T21:30:14.978198Z"},"trusted":true},"outputs":[],"source":["base_lpo_fold = 2\n","small_size = 50\n","mid_size = small_size * 2\n","large_size = mid_size * 10\n","\n","from sklearn.model_selection import StratifiedKFold\n","\n","def get_lpo_fold(): \n","    lpo_fold = base_lpo_fold if len(X) >base_lpo_fold else len(X)-1\n","    return lpo_fold\n","\n","# Using to get the correct cv for a given dataset\n","def get_cross_validation(X):\n","    l = len(X)\n","    if l <= small_size:\n","        return LeavePOut(get_lpo_fold())\n","    if l <=mid_size:\n","        return LeaveOneOut()\n","    if l <=large_size:\n","        classes = np.unique(X.iloc[:,-1])\n","        if len(classes) <10:\n","            return KFold(n_splits=10)\n","        return StratifiedKFold(n_splits=10)\n","    classes = np.unique(X.iloc[:,-1])\n","    if len(classes) <5:\n","        return KFold(n_splits=5)\n","    return StratifiedKFold(n_splits=5)\n","        \n","def get_cross_validation_name(X):\n","    l = len(X)\n","    if l <= 50:\n","        return \"LeavePOut\"\n","    if l <=100:\n","        return \"LeaveOneOut\"\n","    if l <=1000:\n","        return \"KFold 10\"\n","    return \"KFold 5\"\n","\n","def get_cross_validation_num_fold(X):\n","    l = len(X)\n","    if l <= 50:\n","        return get_lpo_fold()\n","    if l <=100:\n","        return 1\n","    if l <=1000:\n","        return 10\n","    return 5\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.006919Z","iopub.status.busy":"2022-10-05T21:30:15.005733Z","iopub.status.idle":"2022-10-05T21:30:15.018432Z","shell.execute_reply":"2022-10-05T21:30:15.017193Z","shell.execute_reply.started":"2022-10-05T21:30:15.006846Z"},"trusted":true},"outputs":[],"source":["def get_name(file_path):\n","    return file_path.split(\".\")[2].split(\"/\")[-1]\n","\n","datasets = [\n","    ('../input/bioconductor/COPDSexualDimorphism.data.csv',True,True,True),\n","    ('../input/bioconductor/bcellViper.csv',True,True,True),\n","    ('../input/bioconductor/bladderbatch.csv',True,True,True),\n","    ('../input/bioconductor/ALL.csv',True,True,True),\n","    \n","    ('../input/bioconductor/SRBCT.arff',False,False,False),\n","    ('../input/bioconductor/Lymphoma.arff',False,False,False),\n","    ('../input/bioconductor/Breast.arff',False,False,False),\n","    \n","    \n","    ('../input/microbiomic/CS.csv',False,False,False,True),\n","    ('../input/microbiomic/CSS.csv',False,False,False,True),\n","    ('../input/microbiomic/FS.csv',False,False,False,True),\n","    ('../input/microbiomic/FSH.csv',False,False,False,False),\n","    ('../input/microbiomic/CBH.csv',False,False,False,True),\n","    \n","    ('../input/sickitmat/ALLAML.mat',False,False,False),\n","    ('../input/sickitmat/Carcinom.mat',False,False,False),\n","    ('../input/sickitmat/pixraw10P.mat',False,False,False),\n","    ('../input/sickitmat/Prostate-GE.mat',False,False,False),\n","    ('../input/sickitmat/SMK-CAN-187.mat',False,False,False),\n","    \n","    \n","    ('../input/microarray/chin.csv',False,False,False),\n","    ('../input/microarray/su.csv',False,False,False),\n","    ('../input/microarray/yeoh.csv',False,False,False),\n","]\n"]},{"cell_type":"markdown","metadata":{},"source":["# K-Values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.022757Z","iopub.status.busy":"2022-10-05T21:30:15.021223Z","iopub.status.idle":"2022-10-05T21:30:15.036308Z","shell.execute_reply":"2022-10-05T21:30:15.035541Z","shell.execute_reply.started":"2022-10-05T21:30:15.022703Z"},"trusted":true},"outputs":[],"source":["k_values = [1,2,3,4,5,10,15,20,25,30,50,100]"]},{"cell_type":"markdown","metadata":{},"source":["# Models and Measurments"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.038143Z","iopub.status.busy":"2022-10-05T21:30:15.037650Z","iopub.status.idle":"2022-10-05T21:30:15.057864Z","shell.execute_reply":"2022-10-05T21:30:15.056119Z","shell.execute_reply.started":"2022-10-05T21:30:15.038103Z"},"trusted":true},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVR\n","from sklearn.naive_bayes import GaussianNB\n","\n","models = [\n","    KNeighborsClassifier(5),\n","    RandomForestClassifier(),\n","    LogisticRegression(),\n","    SVC(probability=True),\n","    GaussianNB(),\n","]\n","models_names = [\n","    'KNN',\n","    'Random Forest',\n","    'Logistic Regression',\n","    'SVM',\n","    'NB',\n","]"]},{"cell_type":"markdown","metadata":{},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.061400Z","iopub.status.busy":"2022-10-05T21:30:15.061033Z","iopub.status.idle":"2022-10-05T21:30:15.079651Z","shell.execute_reply":"2022-10-05T21:30:15.078523Z","shell.execute_reply.started":"2022-10-05T21:30:15.061373Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import auc\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import classification_report\n","\n","\n","\n","def pr_auc(y, y_pred_proba,**kwargs):\n","    if 0 not in y:\n","        y = [x-1 for x in y]\n","    classes = np.unique(y)\n","    def calc_pr_auc(y,y_pred_proba,class_index):\n","        pos_class_probas = [pred_probas[class_index] for pred_probas in y_pred_proba]\n","        class_num = classes[class_index]\n","        y_binary = [1 if x == class_num else 0 for x in y]\n","        precision, recall, thresh = precision_recall_curve(y_binary,pos_class_probas)\n","        pr_auc = auc(sorted(precision),sorted(recall)) # calc auc of precision recall curve\n","        return pr_auc\n","    \n","    final_pr_auc = 0\n","    for i in range(len(classes)):\n","        final_pr_auc += calc_pr_auc(y,y_pred_proba,i)\n","    return final_pr_auc / len(classes)\n","\n","def roc_auc(y,y_pred_proba, **kwargs):\n","    classes = np.unique(y)\n","    y_pred_proba = np.array(y_pred_proba)\n","    if len(classes) == 2:\n","        y_pred_proba = [x[1] for x in y_pred_proba]\n","    classes_index = [c-1 for c in classes]\n","    try:\n","        return roc_auc_score(y,y_pred_proba, multi_class='ovo',labels=kwargs[\"labels\"])\n","    except:\n","        return -1\n","    \n","def matthew(y,y_pred, **kwargs):\n","    return matthews_corrcoef(y,y_pred)\n","def acc(y,y_pred,**kwargs):\n","    return accuracy_score(y,y_pred)\n","    \n","from sklearn.metrics import  make_scorer\n","\n","# Returns a generic metric function for a standard API\n","def metric_closure(metric, needs_proba=False):\n","    def apply_metric(estimator, X, y, y_pred=None, y_pred_proba=None,labels=None):\n","        if needs_proba:\n","            if y_pred_proba is None:\n","                y_pred_proba = estimator.predict_proba(X)\n","            return metric(y,y_pred_proba, labels=labels)\n","        else:\n","            if y_pred is None:\n","                y_pred = estimator.predict(X)\n","            return metric(y,y_pred, labels=labels)\n","    return apply_metric\n","        \n","\n","# Some metrics need to receive y_score with more than one class\n","# in the it or an error is thrown.\n","# This is a wrapper function for these metrics\n","def more_than_one_class(metric):\n","    def more_than_one_class1(y,y_pred,labels=None):\n","        return None if len(np.unique(y)) <= 1 else metric(y,y_pred,labels=labels)\n","    return more_than_one_class1\n","\n","metrics = [\n","    metric_closure(more_than_one_class(roc_auc),True), # making sure there is more than 1 classes in y - else not defined\n","    metric_closure(more_than_one_class(acc)),\n","    metric_closure(matthew),\n","    metric_closure(pr_auc,True),\n","\n","]\n","metrics_names = [\n","    \"AUC\",\n","    \"ACC\",\n","    \"MCC\",\n","    \"PR-AUC\",\n","]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Feature selections"]},{"cell_type":"markdown","metadata":{},"source":["# # CL4-FS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.081785Z","iopub.status.busy":"2022-10-05T21:30:15.081390Z","iopub.status.idle":"2022-10-05T21:30:15.110642Z","shell.execute_reply":"2022-10-05T21:30:15.109900Z","shell.execute_reply.started":"2022-10-05T21:30:15.081749Z"},"trusted":true},"outputs":[],"source":["def feature_selection1(X, Y, K):\n","    \"\"\"\n","    Thabtah, Fadi, et al. \"Least Loss: A simplified filter method for feature selection.\"\n","    Information Sciences 534 (2020): 1-15\n","    \"\"\"\n","    from sklearn.metrics import mutual_info_score\n","    import numpy as np\n","    ##Entropy\n","    def entropy(Y):\n","        \"\"\"\n","        Also known as Shanon Entropy\n","        Reference: https://en.wikipedia.org/wiki/Entropy_(information_theory)\n","        \"\"\"\n","        unique, count = np.unique(Y, return_counts=True, axis=0)\n","        prob = count/len(Y)\n","        en = np.sum((-1)*prob*np.log2(prob))\n","        return en\n","\n","\n","    #Joint Entropy\n","    def jEntropy(Y,X):\n","        \"\"\"\n","        H(Y;X)\n","        Reference: https://en.wikipedia.org/wiki/Joint_entropy\n","        \"\"\"\n","        YX = np.c_[Y,X]\n","        return entropy(YX)\n","\n","    #Conditional Entropy\n","    def cEntropy(Y, X):\n","        \"\"\"\n","        conditional entropy = Joint Entropy - Entropy of X\n","        H(Y|X) = H(Y;X) - H(X)\n","        Reference: https://en.wikipedia.org/wiki/Conditional_entropy\n","        \"\"\"\n","        return jEntropy(Y, X) - entropy(X)\n","    \n","    def sort_by_other(other, n):\n","        ans = [0]*n\n","        for i in range(n):\n","            ans[i] = np.argmax(other)\n","            other[np.argmax(other)] = -1\n","        return ans\n","    \n","    num_rows, num_cols = X.shape\n","    MI = [0]*num_cols\n","    NS = [i for i in range(num_cols)]\n","    S = [0]*K\n","    \n","    # initialization\n","    for i in range(num_cols):\n","        MI[i] = mutual_info_score(Y, X[:, i])\n","    NS = sort_by_other(MI.copy(), num_cols)\n","    #choosing K features\n","    for k in range(min(K, num_cols)):\n","        S[k] = np.argmax(MI)\n","        MI[S[k]] = 0\n","        NS.remove(S[k])\n","        # updating the rest of the features\n","        for i in range(num_cols-k-1):\n","            MI[NS[i]] = min(MI[NS[i]], mutual_info_score(Y, X[:, NS[i]]))\n","        NS = sort_by_other(MI.copy(), num_cols-k)\n","            \n","    # ready the answer, for every value in S is a feature we chose so we place 1 for it, otherwise 0\n","    ans = [0]*num_cols\n","    for i in range(K):\n","        ans[S[i]] = 1\n","    return np.array(ans)\n","\n","def feature_selection2(X, Y, K):\n","    \"\"\"\n","    R. Cai, Z. Hao, X. Yang, W. Wen\n","    An efficient gene selection algorithm based on mutual information\n","    Neurocomputing, 72 (4-6) (2009), pp. 991-999\n","\n","    \"\"\"\n","    import numpy as np\n","    \n","    def calc_pxy(X, Y, xi, yi):\n","        match = 0\n","        for i in range(len(X)):\n","            if X[i] == xi and Y[i] == yi:\n","                match += 1\n","        return match/len(X)\n","\n","    def calc_L2(pxy, X, Y, uX, uY):\n","        counter = 0\n","        sum_L2 = 0\n","        for i in range(len(uX)):\n","            xi_prob = np.count_nonzero(X == uX[i]) / len(X)\n","            for j in range(len(uY)):\n","                yj_prob = np.count_nonzero(Y == uY[j]) / len(Y)\n","                sum_L2 += (pxy[counter] - (xi_prob*yj_prob)) ** 2\n","                counter += 1\n","        return sum_L2\n","    \n","    num_rows, num_cols = X.shape\n","    feature_ans = [0] * num_cols\n","    L2_values = [0] * num_cols\n","    y_values = np.unique(Y)\n","    for i in range(num_cols):\n","        column = X[ :,i]\n","        unique_column = np.unique(column)\n","        xiyj = [calc_pxy(column, Y, j, k) for j in unique_column for k in y_values]\n","        L2 = calc_L2(xiyj, column, Y, unique_column, y_values)\n","        L2_values[i] = L2\n","    for i in range(min(K, num_cols)):\n","        max_val = np.amax(L2_values)\n","        arg = np.where(L2_values == max_val)[0][0]\n","        L2_values[arg] = -1\n","        feature_ans[arg] = 1\n","    \n","    return np.array(feature_ans)\n","        \n","\n","def feature_selection_improved(X, Y, K):\n","    \"\"\"\n","    R. Cai, Z. Hao, X. Yang, W. Wen\n","    An efficient gene selection algorithm based on mutual information\n","    Neurocomputing, 72 (4-6) (2009), pp. 991-999\n","\n","    \"\"\"\n","    import numpy as np\n","    \n","    def calc_pxy_improved(X, Y, xi, yi):\n","        match = 0\n","        for i in range(len(X)):\n","            if X[i] == xi and Y[i] == yi:\n","                match += 1\n","        return match/len(X)\n","\n","    def calc_L2_improved(pxy, X, Y, uX, uY):\n","        margin = (np.amax(X) - np.amin(X)) * 0.01\n","        counter = 0\n","        sum_L2 = 0\n","        for i in range(len(uX)):\n","            xi_prob = np.count_nonzero(np.where(np.logical_and((X > uX[i] - margin), (X < uX[i] + margin)))[0])\n","            for j in range(len(uY)):\n","                yj_prob = np.count_nonzero(Y == uY[j]) / len(Y)\n","                sum_L2 += (pxy[counter] - (xi_prob*yj_prob)) ** 2\n","                counter += 1\n","        return sum_L2\n","    \n","    num_rows, num_cols = X.shape\n","    feature_ans = [0] * num_cols\n","    L2_values = [0] * num_cols\n","    y_values = np.unique(Y)\n","    for i in range(num_cols):\n","        column = X[ :,i]\n","        unique_column = np.unique(column)\n","        xiyj = [calc_pxy_improved(column, Y, j, k) for j in unique_column for k in y_values]\n","        L2 = calc_L2_improved(xiyj, column, Y, unique_column, y_values)\n","        L2_values[i] = L2\n","    for i in range(min(K, num_cols)):\n","        max_val = np.amax(L2_values)\n","        arg = np.where(L2_values == max_val)[0][0]\n","        L2_values[arg] = -1\n","        feature_ans[arg] = 1\n","    \n","    return np.array(feature_ans)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.112713Z","iopub.status.busy":"2022-10-05T21:30:15.111816Z","iopub.status.idle":"2022-10-05T21:30:15.148809Z","shell.execute_reply":"2022-10-05T21:30:15.147782Z","shell.execute_reply.started":"2022-10-05T21:30:15.112685Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_selection import RFE\n","from sklearn.feature_selection import f_classif\n","from sklearn.feature_selection import SelectFdr\n","import heapq\n","\n","import ReliefF\n","from skfeature.function.information_theoretical_based.MRMR import mrmr\n","from skfeature.function.similarity_based import reliefF\n","\n","\n","def get_k_biggest(a,K):\n","    best_indexes = heapq.nlargest(K, range(len(a)), a.take)\n","    return best_indexes\n","\n","# Creating a class for each Feature selection algorithm so \n","# it would fit exactly to the pipeline of evaluating the algorithms\n","\n","class mrmrclass:\n","    def __init__(self,K):\n","        self.K= K\n","        self.features_names = []\n","        self.features = None\n","        self.scores_=[]\n","        \n","    def fit(self,X,y):\n","        self.features = mrmr(X.to_numpy(),y.to_numpy(),mode=\"index\",n_selected_features=self.K)\n","        self.scores_=[1 for x in range(len(self.features))]\n","        \n","    def transform(self,X):\n","        new_X = X.iloc[:,self.features]\n","        self.features_names = [name for name in new_X.columns]\n","        return new_X\n","    \n","    def fit_transform(self,X,y):\n","        self.fit(X,y)\n","        return self.transform(X)\n","\n","    def get_scores(self):\n","        return self.scores_ \n","    \n","class reliefFclass:\n","    def __init__(self,K):\n","        self.features_names = []\n","        self.rel = None\n","        self.K = K\n","        self.features = None\n","        self.scores_ = []\n","    \n","    def fit(self,X,y):\n","        self.rel = ReliefF.ReliefF(n_neighbors=5,n_features_to_keep=self.K)\n","        self.rel.fit(X.to_numpy(),y.to_numpy())\n","        self.features = self.rel.top_features[:self.K] \n","        self.features_names = [name for name in X.iloc[:,self.features]]\n","        self.scores_= self.rel.feature_scores[self.features]\n","        \n","    def transform(self,X):\n","        return X.loc[:,self.features_names]\n","    \n","    def fit_transform(self,X,y):\n","        self.fit(X,y)\n","        return self.transform(X)\n","    \n","    def get_scores(self):\n","        return self.scores_ \n","    \n","class RFEclass:\n","    def __init__(self,K):\n","        self.features_names = []\n","        self.rfe = None\n","        self.K = K\n","        self.features = None\n","        self.scores_ = []\n","    \n","    def fit(self,X,y):\n","        self.rfe = RFE(SVR(kernel=\"linear\"), n_features_to_select=self.K,verbose=2)\n","        self.rfe = self.rfe.fit(X,y)\n","        self.features = self.rfe.get_support(True)\n","        self.features_names = [x for x in X.iloc[:,self.features].columns]\n","        self.scores_= [1 for i in range(self.K)]\n","        \n","    def transform(self,X):\n","        return X.iloc[:,self.features]\n","    \n","    def fit_transform(self,X,y):\n","        self.fit(X,y)\n","        return self.transform(X)\n","    \n","    def get_scores(self):\n","        return self.scores_\n","    \n","class FDRclass:\n","    def __init__(self,K):\n","        self.features_names = []\n","        self.fdr=None\n","        self.K = K\n","        self.features = None\n","        self.scores_ = []\n","    \n","    def fit(self,X,y):\n","        self.fdr =  SelectFdr(f_classif, alpha=0.1)\n","        self.fdr.fit(X,y)\n","        self.scores_= self.fdr.scores_\n","        \n","    def transform(self,X):\n","        self.features=np.argsort(self.scores_)[::-1][0:self.K] # Taking top k elements in the scores array\n","        new_X = X.iloc[:,self.features]\n","        self.features_names = [name for name in new_X.columns]\n","        return new_X\n","    \n","    \n","    def fit_transform(self,X,y):\n","        self.fit(X,y)\n","        return self.transform(X)\n","    \n","    def get_scores(self):\n","        return [self.fdr.scores_[i] for i in self.features]\n","    \n","class fs1:\n","    def __init__(self,K):\n","        self.features_names = []\n","        self.K = K\n","        self.features = None\n","        self.scores_ = []\n","    \n","    def fit(self,X,y):\n","        fs_result = feature_selection1(X.to_numpy(),y.to_numpy(),self.K) # result = [0,1,1,0,0,1] (1 we take, 0 we dont)\n","        self.features = [] # saving the indexes inside features\n","        for i,selected in enumerate(fs_result):\n","            if selected == 1:\n","                self.features.append(i)\n","                \n","        self.features_names = np.array(X.columns[self.features])\n","        self.scores_= [1 for _ in range(self.K)]\n","        \n","    def transform(self,X):\n","        return X.loc[:,self.features_names]\n","    \n","    def fit_transform(self,X,y):\n","        self.fit(X,y)\n","        return self.transform(X)\n","    \n","    def get_scores(self):\n","        return self.scores_ \n","    \n","class fs2:\n","    def __init__(self,K):\n","        self.features_names = []\n","        self.K = K\n","        self.features = None\n","        self.scores_ = []\n","    \n","    def fit(self,X,y):\n","        fs_result = feature_selection2(X.to_numpy(),y.to_numpy(),self.K) # result = [0,1,1,0,0,1] (1 we take, 0 we dont)\n","        self.features = [] # saving the indexes inside features\n","        for i,selected in enumerate(fs_result):\n","            if selected == 1:\n","                self.features.append(i)\n","                \n","        self.features_names = np.array(X.columns[self.features])\n","        self.scores_= [1 for _ in range(self.K)]\n","        \n","    def transform(self,X):\n","        return X.loc[:,self.features_names]\n","    \n","    def fit_transform(self,X,y):\n","        self.fit(X,y)\n","        return self.transform(X)\n","    \n","    def get_scores(self):\n","        return self.scores_ \n","    \n","    \n","    \n","class fs2Improved:\n","    def __init__(self,K):\n","        self.features_names = []\n","        self.K = K\n","        self.features = None\n","        self.scores_ = []\n","    \n","    def fit(self,X,y):\n","        fs_result = feature_selection_improved(X.to_numpy(),y.to_numpy(),self.K) # result = [0,1,1,0,0,1] (1 we take, 0 we dont)\n","        self.features = [] # saving the indexes inside features\n","        for i,selected in enumerate(fs_result):\n","            if selected == 1:\n","                self.features.append(i)\n","                \n","        self.features_names = np.array(X.columns[self.features])\n","        self.scores_= [1 for _ in range(self.K)]\n","        \n","    def transform(self,X):\n","        return X.loc[:,self.features_names]\n","    \n","    def fit_transform(self,X,y):\n","        self.fit(X,y)\n","        return self.transform(X)\n","    \n","    def get_scores(self):\n","        return self.scores_ \n","    \n","# All feature selection algorithms\n","fs = [\n","    lambda k: fs1(k),\n","    lambda k: fs2(k),\n","    lambda k: fs2Improved(k),\n","    lambda k: mrmrclass(k),\n","    lambda k: FDRclass(k),\n","    lambda k: reliefFclass(k),\n","    lambda k: RFEclass(k),\n","]\n","fs_names = [\n","    \"mutual_information\",\n","    \"least_loss\",\n","    \"least_loss_improved\",\n","    \"mrmr\",\n","    \"f_classif\",\n","    \"reliefF\",\n","    \"RFE\",\n","]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Reading All Of The Datasets into **dataframes**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.150540Z","iopub.status.busy":"2022-10-05T21:30:15.150271Z","iopub.status.idle":"2022-10-05T21:30:15.167202Z","shell.execute_reply":"2022-10-05T21:30:15.166161Z","shell.execute_reply.started":"2022-10-05T21:30:15.150515Z"},"trusted":true},"outputs":[],"source":["df_arr = [read(*ds) for ds in datasets]"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Selection Methods"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.169913Z","iopub.status.busy":"2022-10-05T21:30:15.169263Z","iopub.status.idle":"2022-10-05T21:30:15.186949Z","shell.execute_reply":"2022-10-05T21:30:15.185224Z","shell.execute_reply.started":"2022-10-05T21:30:15.169864Z"},"trusted":true},"outputs":[],"source":["import time\n","class FoldInfo:\n","    def __init__(self,fold, measure_type, measure_value, selected_features, selected_features_scores,train_time, test_time):\n","        self.fold = fold\n","        self.measure_type = measure_type\n","        self.measure_value = measure_value\n","        self.selected_features = selected_features\n","        self.selected_features_scores = selected_features_scores\n","        self.train_time = train_time\n","        self.test_time = test_time\n","        \n","# selects K-best features for a df, and returns a df instead of np.array\n","def selectKBest_df(X,y,k): # X,y are dataframes!\n","    kb = SelectKBest(k=k)\n","    kb.fit(X,y)\n","    indices = kb.get_support(True)\n","    return X.iloc[:,indices]\n","\n","\n","\n","def create_pipeline(fs,model,preprocess=None):\n","    pipe = Pipeline(\n","    [\n","        (\"fs\", fs),\n","        (\"model\",model),\n","    ])\n","    return pipe\n","\n","def cross_validate_once(model,cv,X,y):\n","    scoring={metrics_names[i]: metrics[i] for i in range(len(metrics))}\n","    return cross_validate(model,X,y,scoring=scoring,cv=cv, verbose=1)\n","\n","def apply_with_time(f):\n","    start_time = time.time()\n","    res = f()\n","    time_took = time.time() - start_time()\n","    return (res,time_took)\n","\n","from tqdm import tqdm\n","# Applying custom CV function given a cv.\n","# Calculating the relevant metrics for the model and return the results in a special object.\n","def apply_cv(model,fs_model,cv,X,y): # X,y are dataFrame!\n","    scoring={metrics_names[i]: metrics[i] for i in range(len(metrics))}\n","    fold_counter = 1\n","    folds_information = [] # FoldInfo array\n","    # for each fold, apply all metrics and save the information needed with FoldInfo object\n","    \n","    \n","    for train_index, test_index in tqdm(cv.split(X,y)):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","        \n","        train_time, test_time, evaluations = evaluate_metrics(model,X_train,y_train,X_test,y_test)\n","        features_names = fs_model.features_names\n","        scores = fs_model.get_scores()\n","        for i in range(len(evaluations)):\n","            measure_type = metrics_names[i]\n","            measure_value = evaluations[i]\n","            folds_information.append(FoldInfo(fold_counter,measure_type,measure_value,features_names,scores,train_time,test_time))\n","        \n","        fold_counter +=1\n","    return folds_information\n","        \n","# returns the evaluation of all metrics for the model\n","def evaluate_metrics(model,X_train,y_train,X_test,y_test):\n","    train_start = time.time()    \n","    model.fit(X_train,y_train)\n","    train_time = time.time() - train_start\n","    \n","    # just measuring the time it takes to predict_proba as requested\n","    test_start = time.time()\n","    y_pred_proba = model.predict_proba(X_test)\n","    test_time = time.time() - test_start\n","    y_pred = model.predict(X_test)\n","    \n","    labels= np.union1d(np.unique(y_train),np.unique(y_test))\n","\n","    evaluations = [metric(model,X_test,y_test,y_pred,y_pred_proba,labels) for metric in metrics] # metrics values array\n","    return train_time, test_time, evaluations\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Starting the pipelining\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.189815Z","iopub.status.busy":"2022-10-05T21:30:15.189160Z","iopub.status.idle":"2022-10-05T21:30:15.206520Z","shell.execute_reply":"2022-10-05T21:30:15.205348Z","shell.execute_reply.started":"2022-10-05T21:30:15.189777Z"},"trusted":true},"outputs":[],"source":["# Evaluating all metrics on all models for a given feature selection algorithm\n","# There is more options of running this function for part D (giving specific K, specific model etc...)\n","def evaluate_fs(fs,fs_name,X,y,ds_name,cv_function,model=None,model_name=None,k_only=None,fs_ready=False):\n","    number_of_samples = len(X)\n","    dataset_name = ds_name\n","    original_number_of_features = len(X.columns)\n","    filtering_algorithm = fs_name\n","    \n","    models1 = models\n","    models_names1 = models_names\n","    if model is not None:\n","        models1 = [model]\n","        models_names1 = [model_name]\n","        \n","    for i in range(len(models1)): #For each model\n","        base_model = models1[i] # model for the classification\n","        learning_algorithm = models_names1[i]\n","        \n","        k_values1 = k_values\n","        if k_only is not None:\n","            k_values1 = [k_only]\n","        for k in k_values1: # For each k (number_of_features)\n","            \n","            number_of_features_selected = k\n","            cv = get_cross_validation(X)\n","            cv_method = get_cross_validation_name(X)\n","            fold = get_cross_validation_num_fold(X)\n","            \n","            # calculate metrics:\n","            if not fs_ready:\n","                fs_model = fs(k)\n","            else:\n","                fs_model = fs\n","            pipeline = create_pipeline(fs_model,base_model)            \n","            \n","            fold_info = cv_function(pipeline,fs_model,cv,X,y)\n","            \n","            for fold in fold_info:\n","                write_to_results(dataset_name,number_of_samples,original_number_of_features,\n","                                filtering_algorithm,learning_algorithm,number_of_features_selected,cv_method,\n","                                fold.fold,fold.measure_type,fold.measure_value,fold.selected_features,fold.selected_features_scores,\n","                                fold.train_time,fold.test_time,)"]},{"cell_type":"markdown","metadata":{},"source":["Running algorithms on all datasets in the datasets array"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.208392Z","iopub.status.busy":"2022-10-05T21:30:15.208116Z","iopub.status.idle":"2022-10-05T21:30:15.224865Z","shell.execute_reply":"2022-10-05T21:30:15.223201Z","shell.execute_reply.started":"2022-10-05T21:30:15.208368Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","for i in tqdm(range(len(fs))): # For each feature selection\n","    for j in range(len(datasets)): # For each dataset\n","        X,y = get_x_y(df_arr[j])\n","        X,y = X.copy(), y.copy()\n","        ds_name = get_name(datasets[j][0])\n","\n","        evaluate_fs(fs[i],fs_names[i],X,y,ds_name,apply_cv)\n","print(\"Finished\")"]},{"cell_type":"markdown","metadata":{},"source":["Part D"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:15.227394Z","iopub.status.busy":"2022-10-05T21:30:15.226727Z","iopub.status.idle":"2022-10-05T21:30:18.770645Z","shell.execute_reply":"2022-10-05T21:30:18.769232Z","shell.execute_reply.started":"2022-10-05T21:30:15.227351Z"},"trusted":true},"outputs":[],"source":["results = pd.read_csv(\"../input/all-results/ds_results_combined2.csv\")\n","\n","ds_names = results[\"Dataset Name\"].unique()\n","# print(ds_names)\n","# print(\"-------------------------------------------------------\")\n","column_names = list(results.columns.values)[2:-1]\n","# print(column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T21:30:18.774005Z","iopub.status.busy":"2022-10-05T21:30:18.772590Z","iopub.status.idle":"2022-10-05T21:30:22.286062Z","shell.execute_reply":"2022-10-05T21:30:22.285337Z","shell.execute_reply.started":"2022-10-05T21:30:18.773946Z"},"trusted":true},"outputs":[],"source":["from sklearn.decomposition import KernelPCA\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.over_sampling import BorderlineSMOTE\n","import random\n","\n","\n","fs_col = \"Filtering Algorithm\"\n","k_col = \"Number of features selected (K)\"\n","n_comp = 5\n","model_col = \"Learning Algorithm\"\n","\n","\n","def apply_cv2(model,fs_model,cv,X,y): # X,y are dataFrame!\n","    scoring={metrics_names[i]: metrics[i] for i in range(len(metrics))}\n","    fold_counter = 1\n","    folds_information = [] # FoldInfo array\n","    # for each fold, apply all metrics and save the information needed with FoldInfo object\n","    \n","    \n","    for train_index, test_index in tqdm(cv.split(X,y)):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","        \n","        X_train, y_train, X_test, y_test = apply_section_d(X_train,y_train,X_test,y_test)\n","        \n","        train_time, test_time, evaluations = evaluate_metrics(model,X_train,y_train,X_test,y_test)\n","        features_names = fs_model.features_names\n","        scores = fs_model.get_scores()\n","        for i in range(len(evaluations)):\n","            measure_type = metrics_names[i]\n","            measure_value = evaluations[i]\n","            folds_information.append(FoldInfo(fold_counter,measure_type,measure_value,features_names,scores,train_time,test_time))\n","        \n","        fold_counter +=1\n","    return folds_information\n","        \n","\n","def get_minimal_samples(y_train):\n","    return y_train.value_counts().min()\n","\n","def apply_section_d(X_train,y_train,X_test,y_test):\n","    # create KernelPCA of kind linear and rbf \n","    transformer1 = KernelPCA(n_components=n_comp, kernel='linear')\n","    transformer2 = KernelPCA(n_components=n_comp, kernel='rbf')\n","    \n","    # use the KernelPCA on the X of the training using fit_tranform\n","    print(\"KernelIPCA linear fit trans on train...\")\n","    X1_train = transformer1.fit_transform(X_train)\n","    print(\"KernelIPCA rbf fit trans on train...\")\n","    X2_train = transformer2.fit_transform(X_train)\n","    print(\"Done fit trans on train!\\n\\n\")\n","    \n","    # Add the result to the new train\n","    X_train = np.concatenate((X_train, X1_train), axis=1)\n","    X_train = np.concatenate((X_train, X2_train), axis=1)\n","\n","    # use the two KernelPCA that were trained on the X of the test using tranform (is this the new test?)\n","    print(\"KernelIPCA linear transform on test...\")\n","    X1_test = transformer1.transform(X_test)\n","    print(\"KernelIPCA rbf transform on test...\")\n","    X2_test = transformer2.transform(X_test)\n","    print(\"Done fit on test!\\n\\n\")\n","    \n","    # add the result to the new test\n","    X_test = np.concatenate((X_test, X1_test), axis=1)\n","    X_test = np.concatenate((X_test, X2_test), axis=1)\n","    \n","    \n","    # extend the X and y of the training using one of the methods in d\n","    print(\"Applying SMOTE...\")\n","    min_samp = get_minimal_samples(y_train) # becuase sometime there is a low representation of some class we need to define the neighbor number as the lowest sample count for SMOTE to work. (becasue neighbors need to be less than number of samples for some class)\n","    kn = min(min_samp - 1, 5)\n","    mn = min(min_samp - 1, 10)\n","    oversample = BorderlineSMOTE(k_neighbors=kn, m_neighbors=mn)\n","    X_train, y_train = oversample.fit_resample(X_train, y_train)\n","    \n","    X_train = pd.DataFrame(X_train)\n","    X_test = pd.DataFrame(X_test)\n","    return X_train, y_train, X_test, y_test\n","\n","for name in ds_names[:10] :\n","    print()\n","    print(\"Computing \",name, \" :\")\n","    print()\n","    relevent_lines = results[results[\"Dataset Name\"] == name]\n","    \n","    auc_lines = relevent_lines[relevent_lines[\"Measure Type\"] == \"AUC\"]\n","    max_lines = (auc_lines[auc_lines[\"Measure Value\"] == (auc_lines[auc_lines[\"Measure Value\"] != None])[\"Measure Value\"].max()])\n","\n","    if max_lines.empty:\n","        max_lines.append(auc_lines.iloc[0])\n","        max_line = auc_lines.iloc[[0]]\n","    else:\n","        max_line = max_lines.iloc[[0]]\n","\n","    df = pd.read_csv(\"../input/allsets/\" + name + \".csv\")\n","\n","    # read the data and divide to X and y\n","    X,y = get_x_y(df)\n","\n","    # use the fs from max line on the dataset and get the new dataset\n","    k = max_line[k_col].values[0]\n","    fs_name = max_line[fs_col].values[0]\n","    fs_index = fs_names.index(fs_name)\n","    fs_algo = fs[fs_index]\n","    \n","    model_name = max_line[model_col].values[0]\n","    model_index = models_names.index(model_name)\n","    model = models[model_index]\n","    print(\"model:,\",model_name,model)\n","\n","\n","    print(f\"{fs_name} fitting k = {k}...\\n\\n\")\n","    t0 = time.time()\n","    fs1 = fs_algo(k)\n","    X = fs1.fit_transform(X,y)\n","    fs_train_time = time.time() - t0\n","        \n","    # activate the training model in the max line on the new training set\n","    evaluate_fs(fs1,fs_name,X,y,name,apply_cv2,model,model_name+\"_Aug\",k,True)\n","    # report the results in according to the right CV algorithm (add the results to the csv of all the results)\n"]},{"cell_type":"markdown","metadata":{},"source":["Part E"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def part_e():    \n","    from scipy import stats\n","    import pandas as pd\n","    import numpy as np\n","    import scikit_posthocs as sp\n","\n","    results = pd.read_csv(\"../input/all-results/ds_results_combined2.csv\")\n","\n","    ds_names = results[\"Dataset Name\"].unique()\n","    column_names = list(results.columns.values)\n","    f_names = results[\"Filtering Algorithm\"].unique()\n","\n","    all_arrays = [0] * len(f_names)\n","\n","    for i in range(len(f_names)):\n","        name = f_names[i]\n","        f_rows = results[results[\"Filtering Algorithm\"] == name]\n","        f_auc_rows = f_rows[f_rows[\"Measure Type\"] == \"AUC\"]\n","        all_arrays[i] = f_auc_rows[\"Measure Value\"].values\n","\n","    for i in range(7):\n","        all_arrays[i] = all_arrays[i].tolist()\n","    all_arrays[2] += all_arrays[0][-5:]\n","\n","    print(stats.friedmanchisquare(all_arrays[0], all_arrays[1], all_arrays[2], all_arrays[3], all_arrays[4], all_arrays[5], all_arrays[6]))\n","\n","    #combine three groups into one array\n","    data = np.array(all_arrays)\n","\n","    #perform Nemenyi post-hoc test\n","    print(sp.posthoc_nemenyi_friedman(data.T))"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"c3f4d5e964f17fb9bbb51f92f4df5817551e467f76ca73f49ed6a6ee17bb446a"}}},"nbformat":4,"nbformat_minor":4}
